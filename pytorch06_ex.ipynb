{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPNizyWWcaEqF7y1/WMkdHX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lanehale/airline-chatbot/blob/main/pytorch06_ex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSPpzrJXpdqh"
      },
      "outputs": [],
      "source": [
        "# For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+\n",
        "\"\"\" We're well beyond those versions now, no need to download nightly versions\n",
        "try:\n",
        "  import torch\n",
        "  import torchvision\n",
        "  assert int(torch.__version__.split(\".\")[1]) >= 12, \"torch version should be 1.12+\"\n",
        "  assert int(torchvision.__version__.split(\".\")[1]) >= 13, \"torchvision version should be 0.13+\"\n",
        "except:\n",
        "  print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n",
        "  !pip3 install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "  import torch\n",
        "  import torchvision\n",
        "\"\"\"\n",
        "import torch\n",
        "import torchvision\n",
        "print(f\"torch version: {torch.__version__}\")\n",
        "print(f\"torchvision version: {torchvision.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Continue with regular imports\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "\n",
        "# Try to get torchinfo, install it if it doesn't work\n",
        "try:\n",
        "  from torchinfo import summary\n",
        "except:\n",
        "  print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
        "  !pip install -q torchinfo\n",
        "  from torchinfo import summary"
      ],
      "metadata": {
        "id": "lrrMVYICqNb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try to import the going_modular directory, download it from GitHub if it doesn't work\n",
        "try:\n",
        "  from going_modular import data_setup, engine\n",
        "except:\n",
        "  \"\"\"\n",
        "  This block attempts to download a GitHub repository,\n",
        "  move a specific directory from the downloaded repository to the current working directory,\n",
        "  and then remove the downloaded repository.\n",
        "  \"\"\"\n",
        "  # Get the going_modular scripts\n",
        "  print(\"[INFO] Couldn't find going_modular scripts... downloading them from GitHub.\")\n",
        "\n",
        "  # Clone the git repository\n",
        "  !git clone https://github.com/lanehale/pytorch-deep-learning\n",
        "\n",
        "  # When cloning a GitHub repository, the directory structure on your local machine doesn't include /tree/main/, so it shouldn't be included in the mv command.\n",
        "  # The . at the end of the command tells mv to move the specified directory into the current working directory.\n",
        "  !mv pytorch-deep-learning/going_modular .\n",
        "\n",
        "  # remove the downloaded repository\n",
        "  !rm -rf pytorch-deep-learning\n",
        "\n",
        "  from going_modular import data_setup, engine"
      ],
      "metadata": {
        "id": "AKHQi2uQsQt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "G5KF5MCns3r2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv going_modular/train.py .\n",
        "!mv going_modular/predict.py .\n",
        "!ls"
      ],
      "metadata": {
        "id": "bT9C_lH4wGKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls going_modular/"
      ],
      "metadata": {
        "id": "vblZtAX2wt0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup device agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "n258grSjqjYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Ex 5. Train the model with more data, say 20% of the images from Food101 of Pizza, Steak and Sushi images\n",
        "      (My get_data.py already uses the larger, 20% dataset)\n",
        "\"\"\"\n",
        "# Get images\n",
        "!python going_modular/get_data.py"
      ],
      "metadata": {
        "id": "iU1C8F2jxAJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "image_path = Path(\"data/pizza_steak_sushi\")\n",
        "\n",
        "# Set up dirs\n",
        "train_dir = image_path / \"train\"\n",
        "test_dir = image_path / \"test\"\n",
        "\n",
        "train_dir, test_dir"
      ],
      "metadata": {
        "id": "KlnAxm0FxLKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a transforms pipeline manually (required for torchvision < 0.13)\n",
        "manual_transforms = transforms.Compose([\n",
        "    transforms.Resize((224,224)),  # 1. Reshape all images to 224x224 (though some models may require different sizes)\n",
        "    transforms.ToTensor(),         # 2. Turn image values to between 0 & 1\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # 3. A mean of [...] across each color channel\n",
        "                         std=[0.229, 0.224, 0.225])   # A standard deviation of [...] across each color channel\n",
        "])"
      ],
      "metadata": {
        "id": "dibl05GgylVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training and testing DataLoaders as well as get a list of class names\n",
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n",
        "    train_dir=train_dir,\n",
        "    test_dir=test_dir,\n",
        "    transform=manual_transforms,  # resize, convert images to between 0 & 1 and normalize them\n",
        "    batch_size=32\n",
        ")\n",
        "train_dataloader, test_dataloader, class_names"
      ],
      "metadata": {
        "id": "3v36Qn2VR8b6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" As of torchvision v0.13+, an automatic transform creation feature has been added. \"\"\"\n",
        "# Get a set of pretrained model weights\n",
        "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT  # .DEFAULT = best available weights from pretraining on ImageNet\n",
        "weights"
      ],
      "metadata": {
        "id": "ze4ZDaK0TBfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "And now to access the transforms associated with our weights, we can use the transforms() method.\n",
        "This is essentially saying \"get the data transforms that were used to train the EfficientNet_B0_Weights on ImageNet\".\n",
        "\"\"\"\n",
        "# Get the transforms used to create our pretrained weights\n",
        "auto_transforms = weights.transforms()\n",
        "auto_transforms"
      ],
      "metadata": {
        "id": "zk6igd1vTnhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "manual_transforms"
      ],
      "metadata": {
        "id": "3rRmMXm3cMMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training and testing DataLoaders as well as get a list of class names\n",
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n",
        "    train_dir=train_dir,\n",
        "    test_dir=test_dir,\n",
        "    transform=auto_transforms,  # perform the same data transforms on our training data as the pretrained model\n",
        "    batch_size=32\n",
        ")\n",
        "train_dataloader, test_dataloader, class_names"
      ],
      "metadata": {
        "id": "0IuqjoGqUFjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OLD: Set up the model with pretrained weights and send it to the target device (this was prior to torchvision v0.13)\n",
        "# model = torchvision.models.efficientnet_b0(pretrained=True).to(device) # OLD method (with pretrained=True)\n",
        "\n",
        "# NEW: Set up the model with pretrained weights and send it to the target device (torchvision v0.13+)\n",
        "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT  # .DEFAULT = best available weights\n",
        "model = torchvision.models.efficientnet_b0(weights=weights).to(device)\n",
        "\n",
        "#model # uncomment to output (it's very long) and see dropout rate and number of in_features"
      ],
      "metadata": {
        "id": "uaWgao4IWlE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print a summary using torchinfo\n",
        "summary(model=model,\n",
        "        input_size=(32, 3, 224, 224),\n",
        "        # col_names=[\"input_size\"], # uncomment for smaller output\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"])"
      ],
      "metadata": {
        "id": "2AF7PS4SY18n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze all base layers in the \"features\" section of the model (the feature extractor) by setting requires_grad=False\n",
        "for param in model.features.parameters():\n",
        "  param.requires_grad = False"
      ],
      "metadata": {
        "id": "shNGECInabSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Adjust the output layer or the classifier portion of our pretrained model to our needs (out_features=3). \"\"\"\n",
        "# Set the manual seeds\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "# Get the length of class_names (one output unit for each class)\n",
        "output_shape = len(class_names)\n",
        "\n",
        "# Recreate the classifier layer and seed it to the target device\n",
        "model.classifier = torch.nn.Sequential(\n",
        "    torch.nn.Dropout(p=0.2, inplace=True),\n",
        "    torch.nn.Linear(in_features=1280,\n",
        "                    out_features=output_shape,  # same number of output units as our number of classes\n",
        "                    bias=True)).to(device)"
      ],
      "metadata": {
        "id": "6ygtUT-hbIzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Do a summary *after* freezing the features and changing the output classifier layer\n",
        "summary(model=model,\n",
        "        input_size=(32, 3, 224, 224),\n",
        "        verbose=0,\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"])"
      ],
      "metadata": {
        "id": "8JKDVqnNcSAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Because we're still working with multi-class classification, we'll use \"CrossEntropyLoss\" as our loss function. \"\"\"\n",
        "# Define loss and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "sU3NysgwduO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Ex 4. Train the model for longer (10 epochs should do)\n",
        "\"\"\"\n",
        "# Set the random seeds\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "# Start the timer\n",
        "from timeit import default_timer as timer\n",
        "start_time = timer()\n",
        "\n",
        "\"\"\" Note: We're only going to be training the parameters classifier here as all of the other parameters in our model have been frozen. \"\"\"\n",
        "# Set up training and save the results (The train() function is in the engine.py script inside the going_modular directory.)\n",
        "results = engine.train(model=model,\n",
        "                       train_dataloader=train_dataloader,\n",
        "                       test_dataloader=test_dataloader,\n",
        "                       optimizer=optimizer,\n",
        "                       loss_fn=loss_fn,\n",
        "                       epochs=10,\n",
        "                       device=device)\n",
        "\n",
        "# End the timer and print out how long it took\n",
        "end_time = timer()\n",
        "print(f\"[INFO] Total training time: {end_time-start_time:.3f} seconds\")"
      ],
      "metadata": {
        "id": "3uGOHPbbfQWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the plot_loss_curves() function from helper_functions.py, download the file if we don't have it\n",
        "try:\n",
        "  from helper_functions import plot_loss_curves\n",
        "except:\n",
        "  print(\"[INFO] Couldn't find helper_functions.py, downloading...\")\n",
        "  with open(\"helper_functions.py\", \"wb\") as f:\n",
        "    import requests\n",
        "    request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
        "    f.write(request.content)\n",
        "  from helper_functions import plot_loss_curves\n",
        "\n",
        "# Plot the loss curves of our model\n",
        "plot_loss_curves(results)"
      ],
      "metadata": {
        "id": "8pqaahwcgtwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Tuple\n",
        "from PIL import Image\n",
        "\n",
        "# 1. Take in a trained model, class names, image path, image size, a transform and target device\n",
        "def pred_and_plot_image(model: torch.nn.Module,\n",
        "                        image_path: str,\n",
        "                        class_names: List[str],\n",
        "                        image_size: Tuple[int, int] = (224, 224),\n",
        "                        transform: torchvision.transforms = None,\n",
        "                        device: torch.device=device):\n",
        "  # 2. Open image\n",
        "  img = Image.open(image_path)\n",
        "\n",
        "  # 3. Create transformation for image (if one doesn't exist)\n",
        "  if transform is not None:\n",
        "    image_transform = transform\n",
        "  else:\n",
        "    image_transform = transforms.Compose([\n",
        "        transforms.Resize(image_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                             std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "  ### Predict on image ###\n",
        "\n",
        "  # 4. Make sure the model is on the target device\n",
        "  model.to(device)\n",
        "\n",
        "  # 5. Turn on model evaluation mode and inference mode\n",
        "  model.eval()\n",
        "  with torch.inference_mode():\n",
        "    # 6. Transform and add an extra dimension to image (model requires samples in [batch_size, color_channels, height, width])\n",
        "    transformed_image = image_transform(img).unsqueeze(dim=0)\n",
        "\n",
        "    # 7. Make a prediction on image with an extra dimension and send it to the target device\n",
        "    target_image_pred = model(transformed_image.to(device))\n",
        "\n",
        "  # 8. Convert logits -> prediction probabilities (using torch.softmax() for multi-class classification)\n",
        "  target_image_pred_probs = torch.softmax(target_image_pred, dim=1)\n",
        "\n",
        "  # 9. Convert prediction probabilities -> prediction labels\n",
        "  target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)\n",
        "\n",
        "  # 10. Plot image with predicted label and probability\n",
        "  plt.figure()\n",
        "  plt.imshow(img)\n",
        "  plt.title(f\"Pred: {class_names[target_image_pred_label]} | Prob: {target_image_pred_probs.max():.3f}\")\n",
        "  plt.axis(False);"
      ],
      "metadata": {
        "id": "hZSpHScMkg0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a random list of image paths from test set\n",
        "import random\n",
        "num_images_to_plot = 5\n",
        "test_image_path_list = list(Path(test_dir).glob(\"*/*.jpg\"))              # get list of all image paths from test data\n",
        "test_image_path_sample = random.sample(population=test_image_path_list,  # go through all of the test image paths\n",
        "                                       k=num_images_to_plot)             # randomly select 'k' image paths to pred and plot\n",
        "\n",
        "# Make predictions on and plot the images\n",
        "for image_path in test_image_path_sample:\n",
        "  pred_and_plot_image(model=model,\n",
        "                      image_path=image_path,\n",
        "                      class_names=class_names,\n",
        "                      # transform=weights.transforms(),  # optionally pass in a specified transform from our pretrained model weights\n",
        "                      image_size=(224, 224))"
      ],
      "metadata": {
        "id": "ADucHjw8qlYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Ex 3. Predict on your own image of pizza/steak/sushi\n",
        "\"\"\"\n",
        "# Get custom images\n",
        "!python going_modular/get_custom_data.py"
      ],
      "metadata": {
        "id": "Wb5npL4JtgAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls data"
      ],
      "metadata": {
        "id": "cUt2rpERtlq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = Path(\"data\")\n",
        "\n",
        "filenames = [\n",
        "    \"cheese-pizza.jpeg\",\n",
        "    \"pizza-slice.jpeg\",\n",
        "    \"pizza-slice2.jpeg\",\n",
        "    \"pizza-sliced.jpeg\",\n",
        "    \"pizza-sliced2.jpeg\",\n",
        "    \"pizza-partial-view.jpeg\",\n",
        "    \"pizza-partial-view2.jpeg\",\n",
        "    \"pizza-side-view.jpeg\"\n",
        "]\n",
        "\n",
        "for f in filenames:\n",
        "  # Set custom image path\n",
        "  custom_image_path = data_path / f\n",
        "  # Predict on custom image\n",
        "  pred_and_plot_image(model=model,\n",
        "                      image_path=custom_image_path,\n",
        "                      class_names=class_names,\n",
        "                      image_size=(224, 224))"
      ],
      "metadata": {
        "id": "DNZvL5xVu60b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "The length of a DataLoader in Python is determined by the number of batches it will produce from the dataset.\n",
        "This number is calculated by dividing the total number of samples in the dataset by the batch size and potentially\n",
        "applying a rounding operation depending on the drop_last argument.\n",
        "\"\"\"\n",
        "len(test_dataloader)  # 150 / 32 = 5"
      ],
      "metadata": {
        "id": "l_LFpDpduXAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Ex 1. Make predictions on the entire test dataset and plot a confusion matrix for the results of our model compared to the truth labels.\n",
        "\"\"\"\n",
        "# Import tqdm for progress bar\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# 1. Make predictions with trained model\n",
        "y_preds = []\n",
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "  for X, y in tqdm(test_dataloader, desc=\"Making predictions\"):\n",
        "    # Send data and targets to target device\n",
        "    X, y = X.to(device), y.to(device)\n",
        "\n",
        "    # Do the forward pass\n",
        "    y_logit = model(X)\n",
        "\n",
        "    # Turn predictions from logits to probabilities to labels\n",
        "    y_pred = torch.softmax(y_logit, dim=1).argmax(dim=1)  # note: perform softmax on the \"logits\" dimension, not \"batch\" dimension\n",
        "                                                          # (in this case we have a batch size of 32, so can perform on dim=1)\n",
        "    # Put predictions on CPU for evaluation\n",
        "    y_preds.append(y_pred.cpu())\n",
        "\n",
        "  # Concatenate list of predictions into a tensor\n",
        "  y_pred_tensor = torch.cat(y_preds)\n",
        "\n",
        "print(y_pred_tensor)\n",
        "y_pred_tensor.shape, len(test_dataloader.dataset)"
      ],
      "metadata": {
        "id": "F3VckdBGzE0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# See if torchmetrics exists, if not, install it\n",
        "try:\n",
        "  import torchmetrics, mlxtend\n",
        "except:\n",
        "  !pip install -q torchmetrics -U mlxtend\n",
        "  import torchmetrics, mlxtend\n",
        "print(f\"mlxtend version: {mlxtend.__version__}\")"
      ],
      "metadata": {
        "id": "IZuTsu4vfKsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchmetrics import ConfusionMatrix\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "\n",
        "# 2. Setup confusion matrix instance and compare predictions to targets\n",
        "confmat = ConfusionMatrix(num_classes=len(class_names), task='multiclass')\n",
        "print(confmat)\n",
        "\n",
        "# Get truth labels for test dataset\n",
        "test_truth = torch.cat([y for X, y in test_dataloader])\n",
        "print(test_truth)\n",
        "\n",
        "# Convert the target list to a tensor\n",
        "target_tensor = torch.tensor(test_dataloader.dataset.targets)\n",
        "\n",
        "confmat_tensor = confmat(preds=y_pred_tensor,\n",
        "                         target=target_tensor)  # Use the converted tensor\n",
        "print(target_tensor)\n",
        "s = \"is\" if torch.equal(test_truth, target_tensor) else \"is NOT\"\n",
        "print(f\"test_truth {s} equal to target_tensor\")\n",
        "#print(torch.eq(test_truth, target_tensor))  # this compares each element in the two tensors\n",
        "print(target_tensor.shape)\n",
        "print(confmat_tensor)\n",
        "\n",
        "# 3. Plot the confusion matrix\n",
        "fig, ax = plot_confusion_matrix(\n",
        "    conf_mat=confmat_tensor.numpy(),\n",
        "    class_names=class_names,\n",
        "    figsize=(10, 7)\n",
        ");"
      ],
      "metadata": {
        "id": "nmKO2GflfR3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Ex 2. Get the \"most wrong\" of the predictions on the test dataset and plot the 5 \"most wrong\" images.\n",
        "\"\"\"\n",
        "# Create a function to return a list of dictionaries with sample, label, prediction, pred prob\n",
        "def predict_and_store(model, test_paths, tranform, class_names, device):\n",
        "  pred_list = []\n",
        "  test_preds = []\n",
        "  for path in test_paths:\n",
        "    # Create empty dict to store info for each sample\n",
        "    pred_dict = {}\n",
        "\n",
        "    # Save sample path\n",
        "    pred_dict[\"image_path\"] = path\n",
        "\n",
        "    # Save class name\n",
        "    class_name = path.parent.stem\n",
        "    pred_dict[\"class_name\"] = class_name\n",
        "\n",
        "    # Save prediction and pred prob\n",
        "    from PIL import Image\n",
        "    img = Image.open(path)\n",
        "    transformed_image = tranform(img).unsqueeze(dim=0).to(device)  # transform image and add batch dimension\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "      pred_logit = model(transformed_image.to(device))\n",
        "      pred_prob = torch.softmax(pred_logit, dim=1)\n",
        "      pred_label = torch.argmax(pred_prob, dim=1)\n",
        "      pred_class = class_names[pred_label.cpu()]  # or can replace .cpu()] with .item\n",
        "\n",
        "      #pred_dict[\"pred_prob\"] = pred_prob.cpu()  # prediction probability\n",
        "      pred_dict[\"pred_prob\"] = pred_prob.max().item()  # Use .item() to get the Python number\n",
        "      pred_dict[\"pred_class\"] = pred_class      # predicted class name\n",
        "\n",
        "      test_preds.append(pred_label.cpu())\n",
        "\n",
        "    # Does the prediction match the true label?\n",
        "    pred_dict[\"correct\"] = class_name == pred_class\n",
        "    # print(pred_dict)\n",
        "\n",
        "    # Add sample dict to list of preds\n",
        "    pred_list.append(pred_dict)\n",
        "\n",
        "    test_preds_tensor = torch.cat(test_preds)\n",
        "\n",
        "  return pred_list, test_preds_tensor"
      ],
      "metadata": {
        "id": "umrrmROKfbR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get list of all image paths from test data\n",
        "from pathlib import Path\n",
        "test_image_path_list = list(Path(test_dir).glob(\"*/*.jpg\"))\n",
        "# test_labels = [path.parent.stem for path in test_image_path_list]\n",
        "\n",
        "simple_tranform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "pred_list, test_preds_tensor = predict_and_store(\n",
        "    model=model,\n",
        "    test_paths=test_image_path_list,\n",
        "    tranform=manual_transforms, # manual_transforms only 9 False, auto_transforms 12 False, #simple_tranform isn't right (many False)\n",
        "    class_names=class_names,\n",
        "    device=device\n",
        ")\n",
        "pred_list[:5], test_preds_tensor"
      ],
      "metadata": {
        "id": "YH-2N5DGCuxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confmat_tensor = confmat(preds=test_preds_tensor,\n",
        "                         target=target_tensor)  # Use the converted tensor\n",
        "\n",
        "s = \"is\" if torch.equal(test_preds_tensor, y_pred_tensor) else \"is NOT\"\n",
        "print(f\"test_preds_tensor {s} equal to y_pred_tensor\")\n",
        "print(test_preds_tensor)\n",
        "print(y_pred_tensor)\n",
        "\n",
        "# 3. Plot the confusion matrix\n",
        "fig, ax = plot_confusion_matrix(\n",
        "    conf_mat=confmat_tensor.numpy(),\n",
        "    class_names=class_names,\n",
        "    figsize=(10, 7)\n",
        ");"
      ],
      "metadata": {
        "id": "xPM-WDJTLV5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn pred_list (test pred dicts) into a DataFrame\n",
        "import pandas as pd\n",
        "test_pred_df = pd.DataFrame(pred_list)\n",
        "\n",
        "# Sort DataFrame by 'correct' then by 'pred_prob'\n",
        "test_pred_df.sort_values(by=[\"correct\", \"pred_prob\"], ascending=[True, False], inplace=True)  # Add inplace=True to modify the DataFrame directly\n",
        "#test_pred_df.head()\n",
        "test_pred_df[:20]"
      ],
      "metadata": {
        "id": "PzMuvL1q3Mtb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the top 5 most wrong images\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "top_5_most_wrong = test_pred_df[:5]\n",
        "\n",
        "for row in top_5_most_wrong.iterrows():\n",
        "  row = row[1]\n",
        "  image_path = row[\"image_path\"]\n",
        "  true_label = row[\"class_name\"]\n",
        "  pred_class = row[\"pred_class\"]\n",
        "  pred_prob = row[\"pred_prob\"]\n",
        "\n",
        "  img = torchvision.io.read_image(str(image_path)).permute(1, 2, 0)  # get image as tensor and permute to [height, width, color_channels]\n",
        "  plt.imshow(img)\n",
        "  plt.title(f\"True: {true_label} | Pred: {pred_class} | Prob: {pred_prob:.3f}\")\n",
        "  plt.axis(False)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "2RjK4g2T6yvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sorting without pandas\n",
        "sorted_pred_list = sorted(pred_list, key=lambda x: (x['correct']==False, x['pred_prob']), reverse=True,)\n",
        "sorted_pred_list[:20]"
      ],
      "metadata": {
        "id": "B8K1ttOZZYLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Ex 6. Try a different model from torchvision.models on the Pizza, Steak, Sushi data\n",
        "\"\"\"\n",
        "weights_B2 = torchvision.models.EfficientNet_B2_Weights.DEFAULT  # .DEFAULT = best available weights from pretraining on ImageNet\n",
        "weights_B2"
      ],
      "metadata": {
        "id": "miJ0jU6YdR2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "auto_transforms_B2 = weights_B2.transforms()\n",
        "auto_transforms_B2"
      ],
      "metadata": {
        "id": "a8-2_rPPd5NL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training and testing DataLoaders and get a list of class names\n",
        "train_dataloader_B2, test_dataloader_B2, class_names_B2 = data_setup.create_dataloaders(\n",
        "    train_dir=train_dir,\n",
        "    test_dir=test_dir,\n",
        "    transform=auto_transforms_B2,  # perform the same data transforms on our training data as the pretrained model\n",
        "    batch_size=32\n",
        ")\n",
        "train_dataloader_B2, test_dataloader_B2, class_names_B2"
      ],
      "metadata": {
        "id": "8OjUjzVXeAHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NEW: Set up the model with pretrained weights and send it to the target device (torchvision v0.13+)\n",
        "model_B2 = torchvision.models.efficientnet_b2(weights=weights_B2).to(device)\n",
        "\n",
        "# View it to see dropout rate and number of in_features\n",
        "#model_B2\n",
        "\"\"\"\n",
        "  (classifier): Sequential(\n",
        "    (0): Dropout(p=0.3, inplace=True)\n",
        "    (1): Linear(in_features=1408, out_features=1000, bias=True)\n",
        "  )\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "BahStD_-ecan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print a summary using torchinfo\n",
        "summary(model=model_B2,\n",
        "        input_size=(32, 3, 224, 224),\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"])"
      ],
      "metadata": {
        "id": "u0UxIQbafJS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze all base layers in the \"features\" section of the model\n",
        "for parm in model_B2.features.parameters():\n",
        "  parm.requires_grad = False"
      ],
      "metadata": {
        "id": "naAY2nqaflMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Adjust the output layer or the classifier portion of our pretrained model to our needs (out_features=3). \"\"\"\n",
        "# Set manual seeds\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "# Get the length of class_names (one output unit for each class)\n",
        "output_shape = len(class_names_B2)\n",
        "\n",
        "# Recreate the classifier layer and seed it to the target device\n",
        "\"\"\"\n",
        "For EfficientNetV2 models, a dropout rate of 0.2 is often a good starting point, as suggested by some EfficientNetV2 developers and other sources.\n",
        "However, the ideal dropout value can depend on factors like the specific model size, the training data, and the regularization strategy used.\n",
        "\"\"\"\n",
        "model_B2.classifier = torch.nn.Sequential(\n",
        "    torch.nn.Dropout(p=0.3, inplace=True),      # find this by displaying model_B2\n",
        "    torch.nn.Linear(in_features=1408,           # also found in the model_B2 view\n",
        "                    out_features=output_shape,  # same number of output units as our number of classes\n",
        "                    bias=True)).to(device)\n",
        "\n",
        "# View the summary after freezing the features and changing the output classifier layer\n",
        "summary(model=model_B2,\n",
        "        input_size=(32, 3, 224, 224),\n",
        "        verbose=0,\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"])"
      ],
      "metadata": {
        "id": "MsqYUhBnf6jq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Train the model \"\"\"\n",
        "# Set the random seeds\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "optimizer_b2 = torch.optim.Adam(model_B2.parameters(), lr=0.001)\n",
        "\n",
        "# Start the timer\n",
        "from timeit import default_timer as timer\n",
        "start_time = timer()\n",
        "\n",
        "\"\"\" Note: We're only going to be training the parameters classifier here as all of the other parameters in our model have been frozen. \"\"\"\n",
        "# Set up training and save the results\n",
        "results_B2 = engine.train(model=model_B2,\n",
        "                          train_dataloader=train_dataloader_B2,\n",
        "                          test_dataloader=test_dataloader_B2,\n",
        "                          optimizer=optimizer_b2,\n",
        "                          loss_fn=loss_fn,\n",
        "                          epochs=10,\n",
        "                          device=device)\n",
        "\n",
        "# End the timer and print out how long it took\n",
        "end_time = timer()\n",
        "print(f\"[INFO] Total running time: {end_time - start_time:.3f} seconds\")"
      ],
      "metadata": {
        "id": "7hDIgIcRhE2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names_B2"
      ],
      "metadata": {
        "id": "NNlrfg2f_Utt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions and store in a list of dictionaries\n",
        "pred_list_B2, test_preds_tensor_B2 = predict_and_store(\n",
        "    model=model_B2,\n",
        "    test_paths=test_image_path_list,\n",
        "    tranform=auto_transforms_B2,\n",
        "    class_names=class_names_B2,\n",
        "    device=device\n",
        ")\n",
        "pred_list_B2[:5], test_preds_tensor_B2"
      ],
      "metadata": {
        "id": "KJayjqrtiqBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup confusion matrix instance and compare predictions to targets\n",
        "confmat = ConfusionMatrix(num_classes=len(class_names_B2), task='multiclass')\n",
        "\n",
        "# Get truth labels for test dataset\n",
        "test_truth_B2 = torch.cat([y for X, y in test_dataloader_B2])\n",
        "\n",
        "confmat_tensor_B2 = confmat(preds=test_preds_tensor_B2,\n",
        "                            target=test_truth_B2)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "fig, ax = plot_confusion_matrix(\n",
        "    conf_mat=confmat_tensor_B2.numpy(),\n",
        "    class_names=class_names_B2,\n",
        "    figsize=(10, 7)\n",
        ");"
      ],
      "metadata": {
        "id": "NcFfbP11kFEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort the list of dictionaries\n",
        "sorted_pred_list_B2 = sorted(pred_list_B2, key=lambda x: (x['correct']==False, x['pred_prob']), reverse=True)\n",
        "\n",
        "# Turn sorted list into a DataFrame of top 5 wrong\n",
        "test_pred_df_B2 = pd.DataFrame(sorted_pred_list_B2[:5])\n",
        "test_pred_df_B2_ = pd.DataFrame(sorted_pred_list_B2[:10])\n",
        "test_pred_df_B2_"
      ],
      "metadata": {
        "id": "FgKiwL7qlMba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the 5 most wrong images\n",
        "for row in test_pred_df_B2.iterrows():\n",
        "  row = row[1]\n",
        "  image_path = row[\"image_path\"]\n",
        "  true_label = row[\"class_name\"]\n",
        "  pred_class = row[\"pred_class\"]\n",
        "  pred_prob = row[\"pred_prob\"]\n",
        "\n",
        "  img = torchvision.io.read_image(str(image_path)).permute(1, 2, 0)  # get image as tensor and permute to [height, width, color_channels]\n",
        "  plt.imshow(img)\n",
        "  plt.title(f\"True: {true_label} | Pred: {pred_class} | Prob: {pred_prob:.4f}\")\n",
        "  plt.axis(False)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "3U1GyrS-l4GE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check results with 20% of data for 10 epochs using eff_b0\n",
        "max(results[\"test_acc\"]), min(results[\"test_loss\"])"
      ],
      "metadata": {
        "id": "MKWUJeZU3WLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check results_B2 with 20% of data for 10 epochs using eff_b2 (double the model parameters)\n",
        "max(results_B2[\"test_acc\"]), min(results_B2[\"test_loss\"])"
      ],
      "metadata": {
        "id": "yb7-SAyY28li"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/get_any_data.py\n",
        "\"\"\"\n",
        "Contains functionality for creating data folders and downloading requested data.\n",
        "\"\"\"\n",
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "def from_path(from_path: str,         # e.g. \"pizza_steak_sushi_20_percent.zip\"\n",
        "              image_dir: str):        # e.g. \"pizza_steak_sushi\"\n",
        "  # Set up path to data folder\n",
        "  data_path = Path(\"data/\")\n",
        "  image_path = data_path / image_dir  # \"pizza_steak_sushi\"\n",
        "\n",
        "  # If the image folder doesn't exist, download it and prepare it...\n",
        "  if image_path.is_dir():\n",
        "    print(f\"{image_path} directory exists.\")\n",
        "  else:\n",
        "    print(f\"Did not find {image_path} directory, creating one...\")\n",
        "    image_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Download images\n",
        "    with open(data_path / from_path, \"wb\") as f:  # \"pizza_steak_sushi_20_percent.zip\"\n",
        "      #url = Path(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/\") / from_path  # Path removes extra slash\n",
        "      url = \"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/\" + from_path\n",
        "      request = requests.get(url)\n",
        "      print(\"Downloading {image_dir} data...\")    # pizza, steak, sushi\n",
        "      f.write(request.content)\n",
        "\n",
        "    # Unzip image data\n",
        "    with zipfile.ZipFile(data_path / from_path, \"r\") as zip_ref:  # \"pizza_steak_sushi_20_percent.zip\"\n",
        "      print(\"Unzipping {image_dir} data...\")      # pizza, steak, sushi\n",
        "      zip_ref.extractall(image_path)\n",
        "\n",
        "    # Remove zip file\n",
        "    os.remove(data_path / from_path)  # \"pizza_steak_sushi_20_percent.zip\""
      ],
      "metadata": {
        "id": "8cfakucS6wFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf data/\n",
        "!ls"
      ],
      "metadata": {
        "id": "MCFq-BJzNF6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls going_modular/"
      ],
      "metadata": {
        "id": "MxrIcK18QG8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from going_modular import get_any_data\n",
        "get_any_data.from_path(from_path=\"pizza_steak_sushi_20_percent.zip\", image_dir=\"pizza_steak_sushi\")"
      ],
      "metadata": {
        "id": "Lokehou5NSbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls data/pizza_steak_sushi"
      ],
      "metadata": {
        "id": "CpVvf4QmRpVY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}