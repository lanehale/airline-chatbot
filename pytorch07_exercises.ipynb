{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPoes7sF4xQxs+dTbw7NNF7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lanehale/airline-chatbot/blob/main/pytorch07_exercises.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Pb_yFTbnfv5"
      },
      "outputs": [],
      "source": [
        "# See if tensorboard exists, if not, install it\n",
        "try:\n",
        "  from torch.utils.tensorboard import SummaryWriter\n",
        "  print(\"SummaryWriter already installed.\")\n",
        "except:\n",
        "  print(\"Installing SummaryWriter...\")\n",
        "  !pip install -q tensorboard\n",
        "  from torch.utils.tensorboard import SummaryWriter\n",
        "  print(\"Done installing SummaryWriter.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Create the going_modular folder and move in its scripts.\n",
        "\"\"\"\n",
        "import os\n",
        "\n",
        "# Try to import the going_modular directory, download it from GitHub if it doesn't work\n",
        "try:\n",
        "  from going_modular import data_setup, engine\n",
        "  print(\"going_modular scripts already downloaded.\")\n",
        "except:\n",
        "  # Get the going_modular scripts\n",
        "  print(\"Downloading going_modular scripts...\")\n",
        "  !git clone https://github.com/lanehale/pytorch-deep-learning\n",
        "  !mv pytorch-deep-learning/going_modular .\n",
        "  !rm -rf pytorch-deep-learning\n",
        "  print(\"going_modular downloaded.\")\n",
        "  from going_modular import data_setup, engine\n",
        "\n",
        "print(\">!ls going_modular\")\n",
        "!ls going_modular"
      ],
      "metadata": {
        "id": "z_23vk38qJl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from pathlib import Path\n",
        "from going_modular import download_data\n",
        "\n",
        "# Download 10 percent and 20 percent training data\n",
        "data_10_percent_path = download_data.from_path(from_path=\"pizza_steak_sushi.zip\", image_dir=\"pizza_steak_sushi\")\n",
        "data_20_percent_path = download_data.from_path(from_path=\"pizza_steak_sushi_20_percent.zip\", image_dir=\"pizza_steak_sushi_20_percent\")\n",
        "\n",
        "# Set up training directory paths\n",
        "train_dir_10_percent = data_10_percent_path / \"train\"\n",
        "train_dir_20_percent = data_20_percent_path / \"train\"\n",
        "\n",
        "# Setup testing directory paths\n",
        "test_dir_10 = data_10_percent_path / \"test\"\n",
        "test_dir_20 = data_20_percent_path / \"test\"\n",
        "\n",
        "test_image_path_list_10 = list(Path(test_dir_10).glob(\"*/*.jpg\"))  # this is only used for predictions\n",
        "test_image_path_list_20 = list(Path(test_dir_20).glob(\"*/*.jpg\"))  # this is only used for predictions"
      ],
      "metadata": {
        "id": "gcomw5NXB55F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Pick a larger model from torchvision.models to add to the experiments"
      ],
      "metadata": {
        "id": "SlskS_5BccTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights_b3 = torchvision.models.EfficientNet_B3_Weights.DEFAULT  # .DEFAULT = best available weights from pretraining on ImageNet\n",
        "weights_b4 = torchvision.models.EfficientNet_B4_Weights.DEFAULT\n",
        "\n",
        "\"\"\"\n",
        "model_b3 = torchvision.models.efficientnet_b3(weights=weights_b3)\n",
        "model_b4 = torchvision.models.efficientnet_b4(weights=weights_b4)\n",
        "\n",
        "print(f\"effnetb3 - {model_b3.classifier}\")\n",
        "print(f\"effnetb4 - {model_b4.classifier}\")\n",
        "\n",
        "effnetb3 - Sequential(\n",
        "  (0): Dropout(p=0.3, inplace=True)\n",
        "  (1): Linear(in_features=1536, out_features=1000, bias=True)\n",
        ")\n",
        "effnetb4 - Sequential(\n",
        "  (0): Dropout(p=0.4, inplace=True)\n",
        "  (1): Linear(in_features=1792, out_features=1000, bias=True)\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "# Create models list (need to create a new model for each experiment)\n",
        "model_parameters = {\"effnetb3\": {\"weights\": weights_b3, \"in_features\": 1536, \"dropout\": 0.3},\n",
        "                    \"effnetb4\": {\"weights\": weights_b4, \"in_features\": 1792, \"dropout\": 0.4},\n",
        "                    }\n",
        "\n",
        "# Create datasets list\n",
        "datasets = [[\"data_10_percent\", train_dir_10_percent, test_dir_10, test_image_path_list_10],\n",
        "            [\"data_20_percent\", train_dir_20_percent, test_dir_20, test_image_path_list_20],\n",
        "            ]\n",
        "\n",
        "# Create epochs list\n",
        "num_epochs = [10]"
      ],
      "metadata": {
        "id": "f06T9DjYulvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\"\"\"\n",
        "Run experiments\n",
        "\"\"\"\n",
        "from torch import nn\n",
        "from going_modular import pretrained_writer as pretrained\n",
        "from going_modular.utils import create_writer, save_model\n",
        "\n",
        "# Setup device agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "optimizer_type = \"Adam\"\n",
        "optimizer_lr = 0.001\n",
        "\n",
        "# Keep track of experiment numbers\n",
        "experiment_number = 12\n",
        "\n",
        "completed_experiments = {}\n",
        "\n",
        "for model_name in model_parameters:\n",
        "  weights = model_parameters[model_name][\"weights\"]\n",
        "  in_features = model_parameters[model_name][\"in_features\"]\n",
        "  dropout = model_parameters[model_name][\"dropout\"]\n",
        "\n",
        "  for data in datasets:\n",
        "    dataset_name = data[0]\n",
        "    train_dir = data[1]\n",
        "    test_dir = data[2]\n",
        "    image_data = data[3]\n",
        "\n",
        "    for epochs in num_epochs:\n",
        "      model_to_train = f\"{experiment_number}_{model_name}_{dataset_name}_{epochs}_epochs\"\n",
        "      model_to_train_str = model_to_train\n",
        "      results = \"results_\" + model_to_train\n",
        "      predictions = \"predictions_\" + model_to_train\n",
        "\n",
        "      if model_name == \"effnetb3\":\n",
        "        model_to_train = torchvision.models.efficientnet_b3(weights=weights_b3).to(device)\n",
        "      elif model_name == \"effnetb4\":\n",
        "        model_to_train = torchvision.models.efficientnet_b4(weights=weights_b4).to(device)\n",
        "\n",
        "      results, predictions = pretrained.run_model_writer(\n",
        "          model=model_to_train,\n",
        "          weights=weights,\n",
        "          train_dir=train_dir,\n",
        "          test_dir=test_dir,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          dropout=dropout,\n",
        "          in_features=in_features,\n",
        "          optimizer_type=optimizer_type,\n",
        "          optimizer_lr=optimizer_lr,\n",
        "          num_epochs=epochs,\n",
        "          image_data=image_data,\n",
        "          device=device,\n",
        "          writer=create_writer(experiment_name=dataset_name,\n",
        "                               model_name=model_name,\n",
        "                               extra=f\"{epochs}_epochs\"),\n",
        "          model_name=model_to_train_str\n",
        "      )\n",
        "\n",
        "      # Save experiment info for later\n",
        "      completed_experiments[experiment_number] = [model_to_train_str, model_to_train, results, predictions]\n",
        "\n",
        "      experiment_number += 1\n",
        "\n",
        "      # Save the model to file so we can get back the best model\n",
        "      save_model(model=model_to_train,\n",
        "                 target_dir=\"models\",\n",
        "                 model_name=model_to_train_str + \".pth\")\n",
        "      print(\"-\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "RJl9uaxsoGKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls models"
      ],
      "metadata": {
        "id": "XAs_tYBYoLww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from going_modular.utils import compare_results\n",
        "\n",
        "for experiment in completed_experiments:\n",
        "  predictions = completed_experiments[experiment][3]\n",
        "  name = completed_experiments[experiment][0]\n",
        "  compare_results(predictions, name, 37)"
      ],
      "metadata": {
        "id": "YeDuw8QkoSdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = 0\n",
        "best_model_index = 0\n",
        "best_model_acc = 0.0\n",
        "\n",
        "for experiment in completed_experiments:\n",
        "  results = completed_experiments[experiment][2]\n",
        "  max_test_acc = max(results['test_acc'])\n",
        "  index += 1\n",
        "\n",
        "  if max_test_acc > best_model_acc:\n",
        "    best_model_acc = max_test_acc\n",
        "    best_model_index = index\n",
        "\n",
        "  print(\n",
        "      f\"{completed_experiments[experiment][0] :<46} | \"\n",
        "      f\"Max test acc: {max_test_acc:.3f} | \"\n",
        "      f\"Min test loss: {min(results['test_loss']):.3f}\"\n",
        "  )"
      ],
      "metadata": {
        "id": "D4h7Igz5obtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the models locally to my machine\n",
        "from google.colab import files\n",
        "\n",
        "#skip = True\n",
        "skip = False\n",
        "\n",
        "if skip:\n",
        "  for experiment in completed_experiments:\n",
        "    model_name = completed_experiments[experiment][0]\n",
        "    model_path = \"models/\" + model_name + \".pth\"  # gets model from here and\n",
        "    files.download(model_path)                    # saves it to my Downloads folder\n",
        "  print(\"Models downloaded.\")\n",
        "else:\n",
        "  print(\"Skipping download of models.\")"
      ],
      "metadata": {
        "id": "41OsaVTLXU8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Introduce data augmentation to the list of experiments"
      ],
      "metadata": {
        "id": "fWu1nRbHd8rg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "weights_b2 = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
        "\n",
        "# Create models list (need to create a new model for each experiment)\n",
        "models = [\"effnetb2_auto\",\n",
        "          \"effnetb2_aug_wide_auto\",\n",
        "          \"effnetb2_aug_many_auto\",\n",
        "          \"effnetb2_norm\",\n",
        "          \"effnetb2_aug_wide_norm\",\n",
        "          \"effnetb2_aug_many_norm\"]\n",
        "\n",
        "model_parameters = {\"weights\": weights_b2, \"in_features\": 1408, \"dropout\": 0.3}\n",
        "\n",
        "# Create datasets list\n",
        "datasets = [[\"data_20_percent\", train_dir_20_percent, test_dir_20, test_image_path_list_20],\n",
        "            ]\n",
        "\n",
        "# Create epochs list\n",
        "num_epochs = [10]"
      ],
      "metadata": {
        "id": "RHdLvITeeIDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\"\"\"\n",
        "Run experiments\n",
        "\"\"\"\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "from going_modular import pretrained_writer as pretrained\n",
        "from going_modular.utils import create_writer, save_model\n",
        "\n",
        "# Setup device agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "optimizer_type = \"Adam\"\n",
        "optimizer_lr = 0.001\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "\n",
        "# Keep track of experiment numbers\n",
        "experiment_number = 16\n",
        "\n",
        "completed_experiments = {}\n",
        "\n",
        "for model_name in models:\n",
        "  weights = model_parameters[\"weights\"]\n",
        "  in_features = model_parameters[\"in_features\"]\n",
        "  dropout = model_parameters[\"dropout\"]\n",
        "\n",
        "  for data in datasets:\n",
        "    dataset_name = data[0]\n",
        "    train_dir = data[1]\n",
        "    test_dir = data[2]\n",
        "    image_data = data[3]\n",
        "\n",
        "    for epochs in num_epochs:\n",
        "      model_to_train = f\"{experiment_number}_{model_name}_{dataset_name}_{epochs}_epochs\"\n",
        "      model_to_train_str = model_to_train\n",
        "      results = \"results_\" + model_to_train\n",
        "      predictions = \"predictions_\" + model_to_train\n",
        "\n",
        "      model_to_train = torchvision.models.efficientnet_b2(weights=weights_b2).to(device)\n",
        "\n",
        "      if model_name == \"effnetb2_auto\":\n",
        "        transform = None  # will default to auto transforms in run_model_writer\n",
        "      elif model_name == \"effnetb2_aug_wide_auto\":\n",
        "        transform = transforms.Compose([\n",
        "            transforms.TrivialAugmentWide(),\n",
        "            weights.transforms()\n",
        "        ])\n",
        "      elif model_name == \"effnetb2_aug_many_auto\":\n",
        "        transform = torchvision.transforms.Compose([\n",
        "            torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
        "            torchvision.transforms.RandomVerticalFlip(p=0.5),\n",
        "            torchvision.transforms.RandomRotation(degrees=10),\n",
        "            torchvision.transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "            weights.transforms()\n",
        "        ])\n",
        "      elif model_name == \"effnetb2_norm\":\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            normalize\n",
        "        ])\n",
        "      elif model_name == \"effnetb2_aug_wide_norm\":\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.TrivialAugmentWide(),\n",
        "            transforms.ToTensor(),\n",
        "            normalize\n",
        "        ])\n",
        "      elif model_name == \"effnetb2_aug_many_norm\":\n",
        "        transform = torchvision.transforms.Compose([\n",
        "            torchvision.transforms.Resize((224, 224)),\n",
        "            torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
        "            torchvision.transforms.RandomVerticalFlip(p=0.5),\n",
        "            torchvision.transforms.RandomRotation(degrees=10),\n",
        "            torchvision.transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "            torchvision.transforms.ToTensor(),\n",
        "            normalize\n",
        "        ])\n",
        "\n",
        "      results, predictions = pretrained.run_model_writer(\n",
        "          model=model_to_train,\n",
        "          weights=weights,\n",
        "          train_dir=train_dir,\n",
        "          test_dir=test_dir,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          dropout=dropout,\n",
        "          in_features=in_features,\n",
        "          optimizer_type=optimizer_type,\n",
        "          optimizer_lr=optimizer_lr,\n",
        "          num_epochs=epochs,\n",
        "          image_data=image_data,\n",
        "          device=device,\n",
        "          writer=create_writer(experiment_name=dataset_name,\n",
        "                               model_name=model_name,\n",
        "                               extra=f\"{epochs}_epochs\"),\n",
        "          model_name=model_to_train_str,\n",
        "          transform=transform\n",
        "      )\n",
        "\n",
        "      # Save experiment info for later\n",
        "      completed_experiments[experiment_number] = [model_to_train_str, model_to_train, results, predictions]\n",
        "\n",
        "      experiment_number += 1\n",
        "\n",
        "      # Save the model to file so we can get back the best model\n",
        "      save_model(model=model_to_train,\n",
        "                 target_dir=\"models\",\n",
        "                 model_name=model_to_train_str + \".pth\")\n",
        "      print(\"-\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "nGqFYx1ZtqH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from going_modular.utils import compare_results\n",
        "\n",
        "for experiment in completed_experiments:\n",
        "  predictions = completed_experiments[experiment][3]\n",
        "  name = completed_experiments[experiment][0]\n",
        "  compare_results(predictions, name, 51)"
      ],
      "metadata": {
        "id": "7moCLLo_1L3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for experiment in completed_experiments:\n",
        "  results = completed_experiments[experiment][2]\n",
        "  print(\n",
        "      f\"{completed_experiments[experiment][0] :<51} | \"\n",
        "      f\"Max test acc: {max(results['test_acc']):.3f} | \"\n",
        "      f\"Min test loss: {min(results['test_loss']):.3f}\"\n",
        "  )"
      ],
      "metadata": {
        "id": "sCiATPY7Qv0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Scale up the dataset by using the entire Food101 dataset from torchvision.models"
      ],
      "metadata": {
        "id": "r_zsD03fYfLZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Set the root directory where the dataset will be downloaded\n",
        "root_dir = './data'\n",
        "\n",
        "# Download all of the Food101 data\n",
        "datasets.Food101(root=root_dir, download=True)"
      ],
      "metadata": {
        "id": "1VpABKxjY20r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls data/food-101/images -1 | wc -l\n",
        "!ls data/food-101/images/waffles -1 | wc -l"
      ],
      "metadata": {
        "id": "Vgc5WMOr3KiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights_b2 = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
        "auto_transform = weights_b2.transforms()\n",
        "\n",
        "# Create train and test datasets with transforms\n",
        "train_dataset_101 = datasets.Food101(root=root_dir, split='train', transform=auto_transform)\n",
        "test_dataset_101 = datasets.Food101(root=root_dir, split='test', transform=auto_transform)\n",
        "\n",
        "len(train_dataset_101), len(test_dataset_101)"
      ],
      "metadata": {
        "id": "vd3hEmZ6GSJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "If using torchvision.datasets above, the list of all test tensors is already in test_dataset_101.\n",
        "We could use a different predict_and_store to convert tensors back to images. But usually models\n",
        "are validated with part of the test data, and predicted on the rest of the test data. Using the\n",
        "provided Food101 datasets prevents a problem, because the test dataset won't split evenly across\n",
        "classes.\n",
        "\n",
        "If using files, this would be all 101,000 images. We'll need to move 25% of each class' images\n",
        "to test directories to use this method. And in that case we might as well split the test data\n",
        "into validation and test sets.\n",
        "\"\"\"\n",
        "# Get a list of all test images for predictions\n",
        "image_path_list_101 = list(Path(\"data/food-101/images\").glob(\"*/*.jpg\"))"
      ],
      "metadata": {
        "id": "aX0RevyNSmA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(image_path_list_101)"
      ],
      "metadata": {
        "id": "Tufe05o71xtC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function to display each model's file size\n",
        "from pathlib import Path\n",
        "\n",
        "def get_model_size(directory_path):\n",
        "  for root, subdirectories, files in os.walk(directory_path):\n",
        "    for file in files:\n",
        "      model_path = os.path.join(root, file)\n",
        "      model_size = Path(model_path).stat().st_size // (1024 * 1024)\n",
        "      print(f\"Model size of {model_path} trained with 100% data: {model_size} MB\")"
      ],
      "metadata": {
        "id": "JG29kvcSheBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function to copy files to train and test directories\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "def copy_to_split_dirs(source_dir, train_dest_dir, test_dest_dir, n):\n",
        "  cmd = f\"mkdir {train_dest_dir}\"\n",
        "  os.popen(cmd).read()\n",
        "  cmd = f\"mkdir {test_dest_dir}\"\n",
        "  os.popen(cmd).read()\n",
        "\n",
        "  every_n = 1\n",
        "\n",
        "  for file_name in os.listdir(source_dir):\n",
        "    if every_n % n == 0:\n",
        "      shutil.copy(os.path.join(source_dir, file_name), test_dest_dir)\n",
        "    else:\n",
        "      shutil.copy(os.path.join(source_dir, file_name), train_dest_dir)\n",
        "    every_n += 1\n",
        "\n",
        "  output = os.popen(f\"ls {train_dest_dir} -1 | wc -l\").read()\n",
        "  print(f\"{output} images in {train_dest_dir}\")\n",
        "  output = os.popen(f\"ls {test_dest_dir} -1 | wc -l\").read()\n",
        "  print(f\"{output} images in {test_dest_dir}\")"
      ],
      "metadata": {
        "id": "R21uV297yEGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Try three classes (pizza, steak, sushi) with 100% data"
      ],
      "metadata": {
        "id": "9mYRMxGdr0h5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data/food-3\n",
        "!mkdir data/food-3/images\n",
        "!mkdir data/food-3/images/train\n",
        "!mkdir data/food-3/images/test"
      ],
      "metadata": {
        "id": "GdHQLEWutE0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = 1000 / 250  # copy every nth item to test folder\n",
        "\n",
        "source_dir = 'data/food-101/images/pizza'\n",
        "test_dest_dir = 'data/food-3/images/test/pizza'\n",
        "train_dest_dir = 'data/food-3/images/train/pizza'\n",
        "\n",
        "copy_to_split_dirs(source_dir, train_dest_dir, test_dest_dir, n)\n",
        "\n",
        "source_dir = 'data/food-101/images/steak'\n",
        "test_dest_dir = 'data/food-3/images/test/steak'\n",
        "train_dest_dir = 'data/food-3/images/train/steak'\n",
        "\n",
        "copy_to_split_dirs(source_dir, train_dest_dir, test_dest_dir, n)\n",
        "\n",
        "source_dir = 'data/food-101/images/sushi'\n",
        "test_dest_dir = 'data/food-3/images/test/sushi'\n",
        "train_dest_dir = 'data/food-3/images/train/sushi'\n",
        "\n",
        "copy_to_split_dirs(source_dir, train_dest_dir, test_dest_dir, n)"
      ],
      "metadata": {
        "id": "Jrt8ZiCAqsmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define data directories like usual\n",
        "train_dir_3 = 'data/food-3/images/train'\n",
        "test_dir_3 = 'data/food-3/images/test'\n",
        "test_images_list_3 = list(Path(test_dir_3).glob(\"*/*.jpg\"))\n",
        "\n",
        "# Create models list\n",
        "model_parameters = {\"effnetb2\": {\"weights\": weights_b2, \"in_features\": 1408, \"dropout\": 0.3},\n",
        "                    }\n",
        "\n",
        "# Create data paths list\n",
        "data_paths = [[\"data_food_3_dir\", train_dir_3, test_dir_3, test_images_list_3],\n",
        "               ]\n",
        "\n",
        "# Create epochs list\n",
        "num_epochs = [1]\n",
        "\n",
        "experiment_number = 28"
      ],
      "metadata": {
        "id": "VSQS4daQ_fSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\"\"\"\n",
        "Run experiments\n",
        "\"\"\"\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "from going_modular import pretrained_writer as pretrained\n",
        "from going_modular.utils import create_writer, save_model\n",
        "\n",
        "# Setup device agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "optimizer_type = \"Adam\"\n",
        "optimizer_lr = 0.001\n",
        "\n",
        "# Keep track of experiment numbers\n",
        "#experiment_number = 24\n",
        "\n",
        "completed_experiments = {}\n",
        "\n",
        "for model_name in model_parameters:\n",
        "  weights = model_parameters[model_name][\"weights\"]\n",
        "  in_features = model_parameters[model_name][\"in_features\"]\n",
        "  dropout = model_parameters[model_name][\"dropout\"]\n",
        "\n",
        "  for data in data_paths:\n",
        "    dataset_name = data[0]\n",
        "    train_dir = data[1]\n",
        "    test_dir = data[2]\n",
        "    image_data = data[3]\n",
        "\n",
        "    for epochs in num_epochs:\n",
        "      model_to_train = f\"{experiment_number}_{model_name}_{dataset_name}_{epochs}_epochs\"\n",
        "      model_to_train_str = model_to_train\n",
        "      results = \"results_\" + model_to_train\n",
        "      predictions = \"predictions_\" + model_to_train\n",
        "\n",
        "      model_to_train = torchvision.models.efficientnet_b2(weights=weights_b2).to(device)\n",
        "\n",
        "      results, predictions = pretrained.run_model_writer(\n",
        "          model=model_to_train,\n",
        "          weights=weights,\n",
        "          train_dir=train_dir,\n",
        "          test_dir=test_dir,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          dropout=dropout,\n",
        "          in_features=in_features,\n",
        "          optimizer_type=optimizer_type,\n",
        "          optimizer_lr=optimizer_lr,\n",
        "          num_epochs=epochs,\n",
        "          image_data=image_data,\n",
        "          device=device,\n",
        "          writer=create_writer(experiment_name=dataset_name,\n",
        "                               model_name=model_name,\n",
        "                               extra=f\"{epochs}_epochs\"),\n",
        "          model_name=model_to_train_str\n",
        "      )\n",
        "\n",
        "      # Save experiment info for later\n",
        "      completed_experiments[experiment_number] = [model_to_train_str, model_to_train, results, predictions]\n",
        "\n",
        "      experiment_number += 1\n",
        "\n",
        "      # Save the model to file so we can get back the best model\n",
        "      save_model(model=model_to_train,\n",
        "                 target_dir=\"models\",\n",
        "                 model_name=model_to_train_str + \".pth\")\n",
        "      print(\"-\"*50 + \"\\n\")\n",
        "\n",
        "### end of experiments ###\n",
        "\n",
        "print()\n",
        "\n",
        "# Display results\n",
        "from going_modular.utils import compare_results\n",
        "\n",
        "for experiment in completed_experiments:\n",
        "  predictions = completed_experiments[experiment][3]\n",
        "  name = completed_experiments[experiment][0]\n",
        "  compare_results(predictions, name, 40)\n",
        "\n",
        "print()\n",
        "\n",
        "# Display the model file size\n",
        "from pathlib import Path\n",
        "\n",
        "directory_path = \"models\"\n",
        "get_model_size(directory_path)"
      ],
      "metadata": {
        "id": "2YCNeSV4_9CX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run the full Food101 dataset"
      ],
      "metadata": {
        "id": "jirn72-XaU41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "def move_to_split_dirs(source_dir, train_dest_dir, test_dest_dir, total_items, num_to_move, split=False):\n",
        "  cmd = f\"mkdir {train_dest_dir}\"\n",
        "  os.popen(cmd).read()\n",
        "  cmd = f\"mkdir {test_dest_dir}\"\n",
        "  os.popen(cmd).read()\n",
        "\n",
        "  n = total_items // num_to_move  # move every nth item to test_dest folder\n",
        "  every_n = 1\n",
        "  count = 0\n",
        "\n",
        "  for file_name in os.listdir(source_dir):\n",
        "    if every_n % n == 0 and count < num_to_move:\n",
        "      shutil.move(os.path.join(source_dir, file_name), test_dest_dir)\n",
        "      count += 1\n",
        "    elif split:\n",
        "      shutil.move(os.path.join(source_dir, file_name), train_dest_dir)\n",
        "    every_n += 1\n",
        "\n",
        "  output = os.popen(f\"ls {train_dest_dir} -1 | wc -l\").read()\n",
        "  print(f\"{output} images in {train_dest_dir}\")\n",
        "  output = os.popen(f\"ls {test_dest_dir} -1 | wc -l\").read()\n",
        "  print(f\"{output} images in {test_dest_dir}\")"
      ],
      "metadata": {
        "id": "Q3RSMqbEae4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data/food-101/images/train\n",
        "!mkdir data/food-101/images/test\n",
        "!mkdir data/food-101/images/val"
      ],
      "metadata": {
        "id": "BdI06G0PbK93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Each class 700 train, 175 validation, 125 test images"
      ],
      "metadata": {
        "id": "VMldgVVWf5Dy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate data into train and test directories\n",
        "for class_name in train_dataset_101.classes:\n",
        "\n",
        "  source_dir = f'data/food-101/images/{class_name}'\n",
        "  test_dest_dir = f'data/food-101/images/val/{class_name}'\n",
        "  train_dest_dir = f'data/food-101/images/train/{class_name}'\n",
        "\n",
        "  # Move 300 to validation\n",
        "  move_to_split_dirs(source_dir, train_dest_dir, test_dest_dir, 1000, 300, True)"
      ],
      "metadata": {
        "id": "twIbCvZkbcbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate test data into validation and test directories\n",
        "for class_name in train_dataset_101.classes:\n",
        "\n",
        "  source_dir = f'data/food-101/images/val/{class_name}'\n",
        "  test_dest_dir = f'data/food-101/images/test/{class_name}'\n",
        "  train_dest_dir = f'data/food-101/images/val/{class_name}'\n",
        "\n",
        "  # Move 125 to test\n",
        "  move_to_split_dirs(source_dir, train_dest_dir, test_dest_dir, 300, 125)"
      ],
      "metadata": {
        "id": "K6rEzmYdbfsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_101 = \"data/food-101/images/train\"\n",
        "val_data_101 = \"data/food-101/images/val\"\n",
        "test_data_101 = \"data/food-101/images/test\"\n",
        "test_data_path_list_101 = list(Path(test_data_101).glob(\"*/*.jpg\"))\n",
        "\n",
        "!ls {train_data_101} -1 | wc -l\n",
        "!ls {val_data_101} -1 | wc -l\n",
        "!ls {test_data_101} -1 | wc -l\n",
        "len(test_data_path_list_101)"
      ],
      "metadata": {
        "id": "TPEVu_lTbk_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create models list\n",
        "model_parameters = {\"effnetb2\": {\"weights\": weights_b2, \"in_features\": 1408, \"dropout\": 0.3},\n",
        "                    }\n",
        "\n",
        "# Create data paths list\n",
        "data_paths = [[\"data_food_101\", train_data_101, val_data_101, test_data_path_list_101],\n",
        "              ]\n",
        "\n",
        "# Create epochs list\n",
        "num_epochs = [5]\n",
        "\n",
        "experiment_number = 24"
      ],
      "metadata": {
        "id": "V9T6Vungbo6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\"\"\"\n",
        "Run experiments\n",
        "\"\"\"\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "from going_modular import pretrained_writer as pretrained\n",
        "from going_modular.utils import create_writer, save_model\n",
        "\n",
        "# Setup device agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "optimizer_type = \"Adam\"\n",
        "optimizer_lr = 0.001\n",
        "\n",
        "# Keep track of experiment numbers\n",
        "#experiment_number = 24\n",
        "\n",
        "completed_experiments = {}\n",
        "\n",
        "for model_name in model_parameters:\n",
        "  weights = model_parameters[model_name][\"weights\"]\n",
        "  in_features = model_parameters[model_name][\"in_features\"]\n",
        "  dropout = model_parameters[model_name][\"dropout\"]\n",
        "\n",
        "  for data in data_paths:\n",
        "    dataset_name = data[0]\n",
        "    train_dir = data[1]\n",
        "    test_dir = data[2]\n",
        "    image_data = data[3]\n",
        "\n",
        "    for epochs in num_epochs:\n",
        "      model_to_train = f\"{experiment_number}_{model_name}_{dataset_name}_{epochs}_epochs\"\n",
        "      model_to_train_str = model_to_train\n",
        "      results = \"results_\" + model_to_train\n",
        "      predictions = \"predictions_\" + model_to_train\n",
        "\n",
        "      model_to_train = torchvision.models.efficientnet_b2(weights=weights_b2).to(device)\n",
        "\n",
        "      results, predictions = pretrained.run_model_writer(\n",
        "          model=model_to_train,\n",
        "          weights=weights,\n",
        "          train_dir=train_dir,\n",
        "          test_dir=test_dir,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          dropout=dropout,\n",
        "          in_features=in_features,\n",
        "          optimizer_type=optimizer_type,\n",
        "          optimizer_lr=optimizer_lr,\n",
        "          num_epochs=epochs,\n",
        "          image_data=image_data,\n",
        "          device=device,\n",
        "          writer=create_writer(experiment_name=dataset_name,\n",
        "                               model_name=model_name,\n",
        "                               extra=f\"{epochs}_epochs\"),\n",
        "          model_name=model_to_train_str\n",
        "      )\n",
        "\n",
        "      # Save experiment info for later\n",
        "      completed_experiments[experiment_number] = [model_to_train_str, model_to_train, results, predictions]\n",
        "\n",
        "      experiment_number += 1\n",
        "\n",
        "      # Save the model to file so we can get back the best model\n",
        "      save_model(model=model_to_train,\n",
        "                 target_dir=\"models\",\n",
        "                 model_name=model_to_train_str + \".pth\")\n",
        "      print(\"-\"*50 + \"\\n\")\n",
        "\n",
        "### end of experiments ###\n",
        "\n",
        "print()\n",
        "\n",
        "# Display results\n",
        "from going_modular.utils import compare_results\n",
        "\n",
        "for experiment in completed_experiments:\n",
        "  predictions = completed_experiments[experiment][3]\n",
        "  name = completed_experiments[experiment][0]\n",
        "  compare_results(predictions, name, 40)\n",
        "\n",
        "print()\n",
        "\n",
        "# Display the model file size\n",
        "from pathlib import Path\n",
        "\n",
        "directory_path = \"models\"\n",
        "get_model_size(directory_path)"
      ],
      "metadata": {
        "id": "CCejfYOfbt7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Each class 600 train, 200 validation, 200 test images"
      ],
      "metadata": {
        "id": "st4gWP49fe7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate data into train and test directories\n",
        "for class_name in train_dataset_101.classes:\n",
        "\n",
        "  source_dir = f'data/food-101/images/{class_name}'\n",
        "  test_dest_dir = f'data/food-101/images/val/{class_name}'\n",
        "  train_dest_dir = f'data/food-101/images/train/{class_name}'\n",
        "\n",
        "  # Move 400 to validation\n",
        "  move_to_split_dirs(source_dir, train_dest_dir, test_dest_dir, 1000, 400, True)"
      ],
      "metadata": {
        "id": "foYI6GYjI2DD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate test data into validation and test directories\n",
        "for class_name in train_dataset_101.classes:\n",
        "\n",
        "  source_dir = f'data/food-101/images/val/{class_name}'\n",
        "  test_dest_dir = f'data/food-101/images/test/{class_name}'\n",
        "  train_dest_dir = f'data/food-101/images/val/{class_name}'\n",
        "\n",
        "  # Move 200 to test\n",
        "  move_to_split_dirs(source_dir, train_dest_dir, test_dest_dir, 400, 200)"
      ],
      "metadata": {
        "id": "Y7VYNVqyI-cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_101 = \"data/food-101/images/train\"\n",
        "val_data_101 = \"data/food-101/images/val\"\n",
        "test_data_101 = \"data/food-101/images/test\"\n",
        "test_data_path_list_101 = list(Path(test_data_101).glob(\"*/*.jpg\"))\n",
        "\n",
        "!ls {train_data_101} -1 | wc -l\n",
        "!ls {val_data_101} -1 | wc -l\n",
        "!ls {test_data_101} -1 | wc -l\n",
        "len(test_data_path_list_101)"
      ],
      "metadata": {
        "id": "tvgcA6kjJaHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create models list\n",
        "model_parameters = {\"effnetb2\": {\"weights\": weights_b2, \"in_features\": 1408, \"dropout\": 0.3},\n",
        "                    }\n",
        "\n",
        "# Create data paths list\n",
        "data_paths = [[\"data_food_101\", train_data_101, val_data_101, test_data_path_list_101],\n",
        "              ]\n",
        "\n",
        "# Create epochs list\n",
        "num_epochs = [5]\n",
        "\n",
        "experiment_number = 25"
      ],
      "metadata": {
        "id": "1d_zOW1uJOGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\"\"\"\n",
        "Run experiments\n",
        "\"\"\"\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "from going_modular import pretrained_writer as pretrained\n",
        "from going_modular.utils import create_writer, save_model\n",
        "\n",
        "# Setup device agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "optimizer_type = \"Adam\"\n",
        "optimizer_lr = 0.001\n",
        "\n",
        "# Keep track of experiment numbers\n",
        "#experiment_number = 24\n",
        "\n",
        "completed_experiments = {}\n",
        "\n",
        "for model_name in model_parameters:\n",
        "  weights = model_parameters[model_name][\"weights\"]\n",
        "  in_features = model_parameters[model_name][\"in_features\"]\n",
        "  dropout = model_parameters[model_name][\"dropout\"]\n",
        "\n",
        "  for data in data_paths:\n",
        "    dataset_name = data[0]\n",
        "    train_dir = data[1]\n",
        "    test_dir = data[2]\n",
        "    image_data = data[3]\n",
        "\n",
        "    for epochs in num_epochs:\n",
        "      model_to_train = f\"{experiment_number}_{model_name}_{dataset_name}_{epochs}_epochs\"\n",
        "      model_to_train_str = model_to_train\n",
        "      results = \"results_\" + model_to_train\n",
        "      predictions = \"predictions_\" + model_to_train\n",
        "\n",
        "      model_to_train = torchvision.models.efficientnet_b2(weights=weights_b2).to(device)\n",
        "\n",
        "      results, predictions = pretrained.run_model_writer(\n",
        "          model=model_to_train,\n",
        "          weights=weights,\n",
        "          train_dir=train_dir,\n",
        "          test_dir=test_dir,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          dropout=dropout,\n",
        "          in_features=in_features,\n",
        "          optimizer_type=optimizer_type,\n",
        "          optimizer_lr=optimizer_lr,\n",
        "          num_epochs=epochs,\n",
        "          image_data=image_data,\n",
        "          device=device,\n",
        "          writer=create_writer(experiment_name=dataset_name,\n",
        "                               model_name=model_name,\n",
        "                               extra=f\"{epochs}_epochs\"),\n",
        "          model_name=model_to_train_str\n",
        "      )\n",
        "\n",
        "      # Save experiment info for later\n",
        "      completed_experiments[experiment_number] = [model_to_train_str, model_to_train, results, predictions]\n",
        "\n",
        "      experiment_number += 1\n",
        "\n",
        "      # Save the model to file so we can get back the best model\n",
        "      save_model(model=model_to_train,\n",
        "                 target_dir=\"models\",\n",
        "                 model_name=model_to_train_str + \".pth\")\n",
        "      print(\"-\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "rv06_VscJG2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from going_modular.utils import compare_results\n",
        "\n",
        "for experiment in completed_experiments:\n",
        "  predictions = completed_experiments[experiment][3]\n",
        "  name = completed_experiments[experiment][0]\n",
        "  compare_results(predictions, name, 40)"
      ],
      "metadata": {
        "id": "xJ0hHmBaVzoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the model file size\n",
        "from pathlib import Path\n",
        "\n",
        "directory_path = \"models\"\n",
        "get_model_size(directory_path)"
      ],
      "metadata": {
        "id": "sG6xiAT_V79Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Each class 750 train, 200 validation, 50 test images"
      ],
      "metadata": {
        "id": "iqKblK0njKrD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate data into train and test directories\n",
        "for class_name in train_dataset_101.classes:\n",
        "\n",
        "  source_dir = f'data/food-101/images/{class_name}'\n",
        "  test_dest_dir = f'data/food-101/images/val/{class_name}'\n",
        "  train_dest_dir = f'data/food-101/images/train/{class_name}'\n",
        "\n",
        "  # Move 250 to validation\n",
        "  move_to_split_dirs(source_dir, train_dest_dir, test_dest_dir, 1000, 250, True)"
      ],
      "metadata": {
        "id": "gwL_xKs9jGHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate test data into validation and test directories\n",
        "for class_name in train_dataset_101.classes:\n",
        "\n",
        "  source_dir = f'data/food-101/images/val/{class_name}'\n",
        "  test_dest_dir = f'data/food-101/images/test/{class_name}'\n",
        "  train_dest_dir = f'data/food-101/images/val/{class_name}'\n",
        "\n",
        "  # Move 50 to test\n",
        "  move_to_split_dirs(source_dir, train_dest_dir, test_dest_dir, 250, 50)"
      ],
      "metadata": {
        "id": "ZdSN_pNcjlGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_101 = \"data/food-101/images/train\"\n",
        "val_data_101 = \"data/food-101/images/val\"\n",
        "test_data_101 = \"data/food-101/images/test\"\n",
        "test_data_path_list_101 = list(Path(test_data_101).glob(\"*/*.jpg\"))\n",
        "\n",
        "!ls {train_data_101} -1 | wc -l\n",
        "!ls {val_data_101} -1 | wc -l\n",
        "!ls {test_data_101} -1 | wc -l\n",
        "len(test_data_path_list_101)"
      ],
      "metadata": {
        "id": "1FjSLfZlj3Ey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create models list\n",
        "model_parameters = {\"effnetb2\": {\"weights\": weights_b2, \"in_features\": 1408, \"dropout\": 0.3},\n",
        "                    }\n",
        "\n",
        "# Create data paths list\n",
        "data_paths = [[\"data_food_101\", train_data_101, val_data_101, test_data_path_list_101],\n",
        "              ]\n",
        "\n",
        "# Create epochs list\n",
        "num_epochs = [5]\n",
        "\n",
        "experiment_number = 26"
      ],
      "metadata": {
        "id": "tG_bEp2vj4Uw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\"\"\"\n",
        "Run experiments\n",
        "\"\"\"\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "from going_modular import pretrained_writer as pretrained\n",
        "from going_modular.utils import create_writer, save_model\n",
        "\n",
        "# Setup device agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "optimizer_type = \"Adam\"\n",
        "optimizer_lr = 0.001\n",
        "\n",
        "# Keep track of experiment numbers\n",
        "#experiment_number = 24\n",
        "\n",
        "completed_experiments = {}\n",
        "\n",
        "for model_name in model_parameters:\n",
        "  weights = model_parameters[model_name][\"weights\"]\n",
        "  in_features = model_parameters[model_name][\"in_features\"]\n",
        "  dropout = model_parameters[model_name][\"dropout\"]\n",
        "\n",
        "  for data in data_paths:\n",
        "    dataset_name = data[0]\n",
        "    train_dir = data[1]\n",
        "    test_dir = data[2]\n",
        "    image_data = data[3]\n",
        "\n",
        "    for epochs in num_epochs:\n",
        "      model_to_train = f\"{experiment_number}_{model_name}_{dataset_name}_{epochs}_epochs\"\n",
        "      model_to_train_str = model_to_train\n",
        "      results = \"results_\" + model_to_train\n",
        "      predictions = \"predictions_\" + model_to_train\n",
        "\n",
        "      model_to_train = torchvision.models.efficientnet_b2(weights=weights_b2).to(device)\n",
        "\n",
        "      results, predictions = pretrained.run_model_writer(\n",
        "          model=model_to_train,\n",
        "          weights=weights,\n",
        "          train_dir=train_dir,\n",
        "          test_dir=test_dir,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          dropout=dropout,\n",
        "          in_features=in_features,\n",
        "          optimizer_type=optimizer_type,\n",
        "          optimizer_lr=optimizer_lr,\n",
        "          num_epochs=epochs,\n",
        "          image_data=image_data,\n",
        "          device=device,\n",
        "          writer=create_writer(experiment_name=dataset_name,\n",
        "                               model_name=model_name,\n",
        "                               extra=f\"{epochs}_epochs\"),\n",
        "          model_name=model_to_train_str\n",
        "      )\n",
        "\n",
        "      # Save experiment info for later\n",
        "      completed_experiments[experiment_number] = [model_to_train_str, model_to_train, results, predictions]\n",
        "\n",
        "      experiment_number += 1\n",
        "\n",
        "      # Save the model to file so we can get back the best model\n",
        "      save_model(model=model_to_train,\n",
        "                 target_dir=\"models\",\n",
        "                 model_name=model_to_train_str + \".pth\")\n",
        "      print(\"-\"*50 + \"\\n\")\n",
        "\n",
        "### end of experiments ###\n",
        "\n",
        "print()\n",
        "\n",
        "# Display results\n",
        "from going_modular.utils import compare_results\n",
        "\n",
        "for experiment in completed_experiments:\n",
        "  predictions = completed_experiments[experiment][3]\n",
        "  name = completed_experiments[experiment][0]\n",
        "  compare_results(predictions, name, 40)\n",
        "\n",
        "print()\n",
        "\n",
        "# Display the model file size\n",
        "from pathlib import Path\n",
        "\n",
        "directory_path = \"models\"\n",
        "get_model_size(directory_path)"
      ],
      "metadata": {
        "id": "tCcRaoq-j-ZW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}