{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMcv0Og2127xr792Le4U4GZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lanehale/airline-chatbot/blob/main/pytorch06_comparisons.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Moved pip installs outside of py scripts as apparently it's best practice.\n",
        "\"\"\"\n",
        "# See if torchmetrics exists, if not, install it\n",
        "try:\n",
        "  import torchmetrics, mlxtend\n",
        "  print(\"torchmetrics already installed.\")\n",
        "except:\n",
        "  print(\"Installing torchmetrics...\")\n",
        "  !pip install -q torchmetrics -U mlxtend\n",
        "  import torchmetrics, mlxtend\n",
        "  print(\"Done installing torchmetrics.\")"
      ],
      "metadata": {
        "id": "gBResdvgY8hT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7N0GcWPTARQ"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Run several transfer learning model experiments with different hyperparameters.\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "Create the going_modular folder and move in its scripts.\n",
        "\"\"\"\n",
        "import os\n",
        "\n",
        "# Try to import the going_modular directory, download it from GitHub if it doesn't work\n",
        "try:\n",
        "  from going_modular import data_setup, engine, get_any_data\n",
        "  print(\"going_modular scripts already downloaded.\")\n",
        "except:\n",
        "  \"\"\"\n",
        "  This block attempts to download a GitHub repository,\n",
        "  move a specific directory from the downloaded repository to the current working directory,\n",
        "  and then remove the downloaded repository.\n",
        "  \"\"\"\n",
        "  # Get the going_modular scripts\n",
        "  print(\"[INFO] Couldn't find going_modular scripts... downloading them from GitHub.\")\n",
        "\n",
        "  # Clone the git repository\n",
        "  !git clone https://github.com/lanehale/pytorch-deep-learning\n",
        "\n",
        "  # When cloning a GitHub repository, the directory structure on your local machine doesn't include /tree/main/, so it shouldn't be included in the mv command.\n",
        "  # The . at the end of the command tells mv to move the specified directory into the current working directory.\n",
        "  !mv pytorch-deep-learning/going_modular .\n",
        "\n",
        "  # remove the downloaded repository\n",
        "  !rm -rf pytorch-deep-learning\n",
        "\n",
        "  # move these two files out to parent directory\n",
        "  !mv going_modular/train.py .\n",
        "  !mv going_modular/predict.py .\n",
        "\n",
        "  from going_modular import data_setup, engine, get_any_data\n",
        "\n",
        "print(\">!ls\")\n",
        "!ls\n",
        "print(\">!ls going_modular\")\n",
        "!ls going_modular"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get 10% dataset\n",
        "get_any_data.from_path(from_path=\"pizza_steak_sushi.zip\", image_dir=\"pizza_steak_sushi\")"
      ],
      "metadata": {
        "id": "ZHL1yjciUIyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from pathlib import Path\n",
        "from going_modular import pretrained_confmat as pretrained\n",
        "\n",
        "# Setup device agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device\n",
        "\n",
        "image_path = Path(\"data/pizza_steak_sushi\")\n",
        "\n",
        "# Set up dirs\n",
        "train_dir = image_path / \"train\"\n",
        "test_dir = image_path / \"test\"    # train/test ratio is actually 67/33 (150/225, 75/225)\n",
        "\n",
        "test_image_path_list = list(Path(test_dir).glob(\"*/*.jpg\"))  # this is only used for predictions\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "dropout = 0.2\n",
        "in_features = 1280\n",
        "optimizer_type = \"Adam\"\n",
        "optimizer_lr = 0.001\n",
        "\n",
        "weights_b0 = torchvision.models.EfficientNet_B0_Weights.DEFAULT  # .DEFAULT = best available weights from pretraining on ImageNet\n",
        "weights_b2 = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
        "\n",
        "\"\"\"\n",
        "Train 10% dataset for 5 epochs using Eff_B0\n",
        "\"\"\"\n",
        "NUM_EPOCHS = 5\n",
        "\n",
        "# Set up the model with pretrained weights and send it to the target device (torchvision v0.13+)\n",
        "model_b0_5x_10p = torchvision.models.efficientnet_b0(weights=weights_b0).to(device)\n",
        "\n",
        "results_b0_5x_10p, pred_list_b0_5x_10p = pretrained.run_model(\n",
        "    model=model_b0_5x_10p,\n",
        "    weights=weights_b0,\n",
        "    train_dir=train_dir,\n",
        "    test_dir=test_dir,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    dropout=dropout,\n",
        "    in_features=in_features,\n",
        "    optimizer_type=optimizer_type,\n",
        "    optimizer_lr=optimizer_lr,\n",
        "    num_epochs=NUM_EPOCHS,\n",
        "    image_data=test_image_path_list,\n",
        "    device=device\n",
        ")"
      ],
      "metadata": {
        "id": "LZaEpYaAjGUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Train 10% dataset for 10 epochs using Eff_B0\n",
        "\"\"\"\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "# Set up the model with pretrained weights and send it to the target device (torchvision v0.13+)\n",
        "model_b0_10x_10p = torchvision.models.efficientnet_b0(weights=weights_b0).to(device)\n",
        "\n",
        "results_b0_10x_10p, pred_list_b0_10x_10p = pretrained.run_model(\n",
        "    model=model_b0_10x_10p,\n",
        "    weights=weights_b0,\n",
        "    train_dir=train_dir,\n",
        "    test_dir=test_dir,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    dropout=dropout,\n",
        "    in_features=in_features,\n",
        "    optimizer_type=optimizer_type,\n",
        "    optimizer_lr=optimizer_lr,\n",
        "    num_epochs=NUM_EPOCHS,\n",
        "    image_data=test_image_path_list,\n",
        "    device=device\n",
        ")"
      ],
      "metadata": {
        "id": "aGBf1ewYo5rJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Train 10% dataset for 5 epochs using Eff_B2\n",
        "\"\"\"\n",
        "dropout = 0.3\n",
        "in_features = 1408\n",
        "NUM_EPOCHS = 5\n",
        "\n",
        "# Set up the model with pretrained weights and send it to the target device (torchvision v0.13+)\n",
        "model_b2_5x_10p = torchvision.models.efficientnet_b2(weights=weights_b2).to(device)\n",
        "\n",
        "results_b2_5x_10p, pred_list_b2_5x_10p = pretrained.run_model(\n",
        "    model=model_b2_5x_10p,\n",
        "    weights=weights_b2,\n",
        "    train_dir=train_dir,\n",
        "    test_dir=test_dir,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    dropout=dropout,\n",
        "    in_features=in_features,\n",
        "    optimizer_type=optimizer_type,\n",
        "    optimizer_lr=optimizer_lr,\n",
        "    num_epochs=NUM_EPOCHS,\n",
        "    image_data=test_image_path_list,\n",
        "    device=device\n",
        ")"
      ],
      "metadata": {
        "id": "9bZgFDq_o2Cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Train 10% dataset for 10 epochs using Eff_B2\n",
        "\"\"\"\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "# Set up the model with pretrained weights and send it to the target device (torchvision v0.13+)\n",
        "model_b2_10x_10p = torchvision.models.efficientnet_b2(weights=weights_b2).to(device)\n",
        "\n",
        "results_b2_10x_10p, pred_list_b2_10x_10p = pretrained.run_model(\n",
        "    model=model_b2_10x_10p,\n",
        "    weights=weights_b2,\n",
        "    train_dir=train_dir,\n",
        "    test_dir=test_dir,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    dropout=dropout,\n",
        "    in_features=in_features,\n",
        "    optimizer_type=optimizer_type,\n",
        "    optimizer_lr=optimizer_lr,\n",
        "    num_epochs=NUM_EPOCHS,\n",
        "    image_data=test_image_path_list,\n",
        "    device=device\n",
        ")"
      ],
      "metadata": {
        "id": "W6W7d1k-ou8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get 20% dataset\n",
        "from going_modular import get_any_data\n",
        "\n",
        "get_any_data.from_path(from_path=\"pizza_steak_sushi_20_percent.zip\", image_dir=\"pizza_steak_sushi_20\")"
      ],
      "metadata": {
        "id": "M_JBp6qJn_uq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = Path(\"data/pizza_steak_sushi_20\")\n",
        "\n",
        "# Set up dirs\n",
        "train_dir = image_path / \"train\"\n",
        "test_dir = image_path / \"test\"    # use 20% test path to maintain 67/33 train/test ratio (300/450, 150/450)\n",
        "\n",
        "test_image_path_list_20 = list(Path(test_dir).glob(\"*/*.jpg\"))  # this is only used for predictions\n",
        "\n",
        "\"\"\"\n",
        "Train 20% dataset for 5 epochs using Eff_B0\n",
        "\"\"\"\n",
        "dropout = 0.2\n",
        "in_features = 1280\n",
        "NUM_EPOCHS = 5\n",
        "\n",
        "# Set up the model with pretrained weights and send it to the target device (torchvision v0.13+)\n",
        "model_b0_5x_20p = torchvision.models.efficientnet_b0(weights=weights_b0).to(device)\n",
        "\n",
        "results_b0_5x_20p, pred_list_b0_5x_20p = pretrained.run_model(\n",
        "    model=model_b0_5x_20p,\n",
        "    weights=weights_b0,\n",
        "    train_dir=train_dir,\n",
        "    test_dir=test_dir,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    dropout=dropout,\n",
        "    in_features=in_features,\n",
        "    optimizer_type=optimizer_type,\n",
        "    optimizer_lr=optimizer_lr,\n",
        "    num_epochs=NUM_EPOCHS,\n",
        "    image_data=test_image_path_list_20,\n",
        "    device=device\n",
        ")"
      ],
      "metadata": {
        "id": "uM1g11qNpy-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Train 20% dataset for 10 epochs using Eff_B0\n",
        "\"\"\"\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "# Set up the model with pretrained weights and send it to the target device (torchvision v0.13+)\n",
        "model_b0_10x_20p = torchvision.models.efficientnet_b0(weights=weights_b0).to(device)\n",
        "\n",
        "results_b0_10x_20p, pred_list_b0_10x_20p = pretrained.run_model(\n",
        "    model=model_b0_10x_20p,\n",
        "    weights=weights_b0,\n",
        "    train_dir=train_dir,\n",
        "    test_dir=test_dir,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    dropout=dropout,\n",
        "    in_features=in_features,\n",
        "    optimizer_type=optimizer_type,\n",
        "    optimizer_lr=optimizer_lr,\n",
        "    num_epochs=NUM_EPOCHS,\n",
        "    image_data=test_image_path_list_20,\n",
        "    device=device\n",
        ")"
      ],
      "metadata": {
        "id": "py2wdnVvqmF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Train 20% dataset for 5 epochs using Eff_B2\n",
        "\"\"\"\n",
        "dropout = 0.3\n",
        "in_features = 1408\n",
        "NUM_EPOCHS = 5\n",
        "\n",
        "# Set up the model with pretrained weights and send it to the target device (torchvision v0.13+)\n",
        "model_b2_5x_20p = torchvision.models.efficientnet_b2(weights=weights_b2).to(device)\n",
        "\n",
        "results_b2_5x_20p, pred_list_b2_5x_20p = pretrained.run_model(\n",
        "    model=model_b2_5x_20p,\n",
        "    weights=weights_b2,\n",
        "    train_dir=train_dir,\n",
        "    test_dir=test_dir,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    dropout=dropout,\n",
        "    in_features=in_features,\n",
        "    optimizer_type=optimizer_type,\n",
        "    optimizer_lr=optimizer_lr,\n",
        "    num_epochs=NUM_EPOCHS,\n",
        "    image_data=test_image_path_list_20,\n",
        "    device=device\n",
        ")"
      ],
      "metadata": {
        "id": "QvUn-Ra-q-gI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Train 20% dataset for 10 epochs using Eff_B2\n",
        "\"\"\"\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "# Set up the model with pretrained weights and send it to the target device (torchvision v0.13+)\n",
        "model_b2_10x_20p = torchvision.models.efficientnet_b2(weights=weights_b2).to(device)\n",
        "\n",
        "results_b2_10x_20p, pred_list_b2_10x_20p = pretrained.run_model(\n",
        "    model=model_b2_10x_20p,\n",
        "    weights=weights_b2,\n",
        "    train_dir=train_dir,\n",
        "    test_dir=test_dir,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    dropout=dropout,\n",
        "    in_features=in_features,\n",
        "    optimizer_type=optimizer_type,\n",
        "    optimizer_lr=optimizer_lr,\n",
        "    num_epochs=NUM_EPOCHS,\n",
        "    image_data=test_image_path_list_20,\n",
        "    device=device\n",
        ")"
      ],
      "metadata": {
        "id": "5Lef6LGQq8yA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "Training the model...\n",
        "100%\n",
        " 10/10 [00:59<00:00,  5.89s/it]\n",
        "Epoch: 1 | train_loss: 0.9791 | train_acc: 0.5604 | test_loss: 0.7128 | test_acc: 0.9011\n",
        "Epoch: 2 | train_loss: 0.7206 | train_acc: 0.8063 | test_loss: 0.5769 | test_acc: 0.9443\n",
        "Epoch: 3 | train_loss: 0.5971 | train_acc: 0.7937 | test_loss: 0.4906 | test_acc: 0.9437\n",
        "Epoch: 4 | train_loss: 0.5227 | train_acc: 0.8271 | test_loss: 0.4484 | test_acc: 0.9222\n",
        "Epoch: 5 | train_loss: 0.4196 | train_acc: 0.8917 | test_loss: 0.3825 | test_acc: 0.9375\n",
        "Epoch: 6 | train_loss: 0.3838 | train_acc: 0.9083 | test_loss: 0.3493 | test_acc: 0.9443\n",
        "Epoch: 7 | train_loss: 0.3517 | train_acc: 0.9208 | test_loss: 0.3165 | test_acc: 0.9688\n",
        "Epoch: 8 | train_loss: 0.3706 | train_acc: 0.9062 | test_loss: 0.3061 | test_acc: 0.9534\n",
        "Epoch: 9 | train_loss: 0.3070 | train_acc: 0.9396 | test_loss: 0.2988 | test_acc: 0.9597\n",
        "Epoch: 10 | train_loss: 0.3613 | train_acc: 0.8958 | test_loss: 0.2692 | test_acc: 0.9750\n",
        "[INFO] Total running time: 59.450 seconds\n",
        "Predicting with image_data...\n",
        "Max test acc: 0.975 | Min test loss: 0.269\n",
        "```\n",
        "link to Confusion Matrix image: https://github.com/lanehale/pytorch-deep-learning/blob/main/pytorch06_comparisons_images/Confusion%20Matrix%20model_b2_10x_20p.png"
      ],
      "metadata": {
        "id": "Fmd5MGD2crf4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function to display results\n",
        "def compare_results(pred_list, name):\n",
        "  false_count = 0\n",
        "  for pred in pred_list:\n",
        "    if pred['correct'] == False:\n",
        "      false_count += 1\n",
        "  false_percent = 100 * false_count / len(pred_list)\n",
        "  print(\n",
        "      f\"{name :<10} | False predictions: {false_count :<2} out of {len(pred_list) :<3}, \"\n",
        "      f\"or {false_percent:5.2f}% wrong, \"\n",
        "      f\"{(100.0 - false_percent):.2f}% right\"\n",
        "  )"
      ],
      "metadata": {
        "id": "hICM2Gu7ruxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compare_results(pred_list_b0_5x_10p, \"b0_5x_10p\")\n",
        "compare_results(pred_list_b2_5x_10p, \"b2_5x_10p\")\n",
        "compare_results(pred_list_b0_10x_10p, \"b0_10x_10p\")\n",
        "compare_results(pred_list_b2_10x_10p, \"b2_10x_10p\")\n",
        "compare_results(pred_list_b0_5x_20p, \"b0_5x_20p\")\n",
        "compare_results(pred_list_b2_5x_20p, \"b2_5x_20p\")\n",
        "compare_results(pred_list_b0_10x_20p, \"b0_10x_20p\")\n",
        "compare_results(pred_list_b2_10x_20p, \"b2_10x_20p\")"
      ],
      "metadata": {
        "id": "lnxesQ61tPMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Max test acc: {max(results_b0_5x_10p['test_acc']):.3f} | Min test loss: {min(results_b0_5x_10p['test_loss']):.3f}\")\n",
        "print(f\"Max test acc: {max(results_b2_5x_10p['test_acc']):.3f} | Min test loss: {min(results_b2_5x_10p['test_loss']):.3f}\")\n",
        "print(f\"Max test acc: {max(results_b0_5x_20p['test_acc']):.3f} | Min test loss: {min(results_b0_5x_20p['test_loss']):.3f}\")\n",
        "print(f\"Max test acc: {max(results_b2_5x_20p['test_acc']):.3f} | Min test loss: {min(results_b2_5x_20p['test_loss']):.3f}\")\n",
        "print()\n",
        "print(f\"Max test acc: {max(results_b0_10x_10p['test_acc']):.3f} | Min test loss: {min(results_b0_10x_10p['test_loss']):.3f}\")\n",
        "print(f\"Max test acc: {max(results_b2_10x_10p['test_acc']):.3f} | Min test loss: {min(results_b2_10x_10p['test_loss']):.3f}\")\n",
        "print(f\"Max test acc: {max(results_b0_10x_20p['test_acc']):.3f} | Min test loss: {min(results_b0_10x_20p['test_loss']):.3f}\")\n",
        "print(f\"Max test acc: {max(results_b2_10x_20p['test_acc']):.3f} | Min test loss: {min(results_b2_10x_20p['test_loss']):.3f}\")"
      ],
      "metadata": {
        "id": "UE8Qa2Sb5UN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained.plot_N_most_wrong(pred_list_b0_5x_10p, n=3)"
      ],
      "metadata": {
        "id": "J7cvnsLD8wll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained.plot_N_most_wrong(pred_list_b2_5x_10p, n=3)"
      ],
      "metadata": {
        "id": "YNjEYc4X9FIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained.plot_N_most_wrong(pred_list_b0_5x_20p, n=3)"
      ],
      "metadata": {
        "id": "NC1TUjAx9Hjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained.plot_N_most_wrong(pred_list_b2_5x_20p, n=3)"
      ],
      "metadata": {
        "id": "Q-tDsbXo9J6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained.plot_N_most_wrong(pred_list_b0_10x_10p, n=3)"
      ],
      "metadata": {
        "id": "1n8UAaEn9MRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained.plot_N_most_wrong(pred_list_b2_10x_10p, n=3)"
      ],
      "metadata": {
        "id": "uBBzN4rv9QUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained.plot_N_most_wrong(pred_list_b0_10x_20p, n=3)"
      ],
      "metadata": {
        "id": "ebtCCmwS9TTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained.plot_N_most_wrong(pred_list_b2_10x_20p, n=3)"
      ],
      "metadata": {
        "id": "SCS24dj69VlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir models\n",
        "torch.save(obj=model_b2_10x_20p.state_dict(),\n",
        "           f=\"models/model_b2_10x_20p.pth\")\n",
        "!ls models"
      ],
      "metadata": {
        "id": "7j736YRYebfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the best model locally to my machine\n",
        "from google.colab import files\n",
        "files.download(\"models/model_b2_10x_20p.pth\")"
      ],
      "metadata": {
        "id": "cQ8SyRs5c5GB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}